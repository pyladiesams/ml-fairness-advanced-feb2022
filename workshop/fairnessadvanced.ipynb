{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Fairness-Aware Classification\n",
    "\n",
    "[Fairlearn](https://fairlearn.org) is a Python toolkit for measuring and optimizing for fairness in machine learning. In this tutorial we will explore several techniques that can be used to advance fairness of a machine learning model.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives**. After this tutorial you will be able to:\n",
    "* apply techniques for fairness-aware classification in Python;\n",
    "* examine strengths and limitations of different fairness-aware classification techniques;\n",
    "\n",
    "--- \n",
    "**WARNING.** *Although the goal of this tutorial is to showcase fair-ml approaches, I would like to stress that **a data scientist's first actions should always include scrutinizing the need for machine learning, the machine learning task formulation, and data collection practices.** Especially in a sensitive application such as pre-trial risk assesment. Moreover, apart from fairness concerns, the accuracy of the models trained in this tutorial are only a bit better than random and likely not particularly useful in practice.*\n",
    "\n",
    "---\n",
    "\n",
    "### COMPAS: A Pre-Trial Risk Assessment Tool\n",
    "COMPAS is a decision support tool used by courts in the United States to assess the likelihood of a defendant becoming a recidivist; i.e., relapses into criminal behavior. In particular, COMPAS risk scores are used in **pre-trial risk assessment**.\n",
    "\n",
    "> #### What is pre-trial risk assessment in the US judicial system?\n",
    "After somebody has been arrested, it will take some time before they go to trial. The primary goal of pre-trial risk assessment is to determine the likelihood that the defendant will re-appear in court at their trial. Based on the assessment, a judge decides whether a defendent will be detained or released while awaiting trial. In case of release, the judge also decides whether bail is set and for which amount. Bail usually takes the form of either a cash payment or a bond. If the defendant can't afford to pay the bail amount in cash - which can be as high as \\$50,000 - they can contract a bondsmen. For a fee, typically around 10\\% of the bail, the bondsmen will post the defendant's bail.\n",
    ">\n",
    "> If the defendant cannot afford bail nor a bail bond, they have to prepare for their trial while in jail. [This](https://eu.clarionledger.com/story/opinion/columnists/2020/05/28/cant-afford-bail-woman-describes-experience-mississippi-bail-fund-collective/5257295002/) [is](https://medium.com/dose/bail-is-so-expensive-it-forces-innocent-people-to-plead-guilty-72a3097a2ebe) [difficult](https://facctconference.org/2018/livestream_vh210.html). The time between getting arrested and a bail hearing can take days, weeks, months, or even years. In some cases, the decision is between pleading guilty and going home. Consequently, people who cannot afford bail are much more likely to plead guilty to a crime they did not commit. \n",
    ">\n",
    "> If the judge's decision is a **false positive**, this has a big impact on the defendant's prospects. On the other extreme, **false negatives** could mean that dangerous individuals are released into society.\n",
    ">\n",
    "Proponents of risk assessment tools argue that they can lead to more efficient, less biased, and more consistent decisions compared to human decision makers. However, concerns have been raised that the scores can replicate historical inequalities.\n",
    "\n",
    "#### Propublica's Analysis of COMPAS\n",
    "In May 2016, investigative journalists of Propublica released a critical analysis of COMPAS. **Propublica's assessment: COMPAS wrongly labeled black defendants as future criminals at almost twice the rate as white defendants**, while white defendants were mislabeled as low risk more often than black defendants ([Propublica, 2016](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)). \n",
    "\n",
    "The analysis of COMPAS is likely one of the most well-known examples of algorithmic bias assessments. Within the machine learning research community, the incident sparked a renewed interest in fairness of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_curve, RocCurveDisplay\n",
    "\n",
    "# fairlearn\n",
    "from fairlearn.metrics import (\n",
    "    false_positive_rate,\n",
    "    false_negative_rate,\n",
    "    true_positive_rate,\n",
    "    MetricFrame,\n",
    "    equalized_odds_difference,\n",
    ")\n",
    "from fairlearn.postprocessing import ThresholdOptimizer, plot_threshold_optimizer\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Pre-Process Data\n",
    "\n",
    "You can download the data set collected by ProPublica [here](https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv). As in the previous tutorial, we pre-process the data similar to ProPublica and select all instances related to Caucasian and African-American defendants. As we intend to train a new classifier, we split the data into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"compas-scores-two-years.csv\")\n",
    "# filter similar to propublica\n",
    "df = df[\n",
    "    (df[\"days_b_screening_arrest\"] <= 30)\n",
    "    & (df[\"days_b_screening_arrest\"] >= -30)\n",
    "    & (df[\"is_recid\"] != -1)\n",
    "    & (df[\"c_charge_degree\"] != \"O\")\n",
    "    & (df[\"score_text\"] != \"N/A\")\n",
    "]\n",
    "# select two largest groups\n",
    "df = df[(df[\"race\"] == \"African-American\") | (df[\"race\"] == \"Caucasian\")]\n",
    "# select columns\n",
    "df = df[\n",
    "    [\n",
    "        \"sex\",\n",
    "        \"age\",\n",
    "        \"race\",\n",
    "        \"priors_count\",\n",
    "        \"juv_fel_count\",\n",
    "        \"juv_misd_count\",\n",
    "        \"juv_other_count\",\n",
    "        \"two_year_recid\",\n",
    "    ]\n",
    "]\n",
    "# convert categorical variables to numerical to make suitable for ML model\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "# define X and y\n",
    "X = df.drop(\"two_year_recid\", axis=1)\n",
    "y = df[\"two_year_recid\"]\n",
    "\n",
    "# split the data in train-validation-test sets; use random_state for reproducibility of the results\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "# inspect dataset\n",
    "display(X_train.head())\n",
    "# proportion of positives\n",
    "print(\"proportion of positives (train): %.2f\" % y_train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data now contains the following features:\n",
    "* *age*. The defendant's age on the COMPAS screening date. \n",
    "* *priors_count*. The number of prior charges up to but not including the current offense.\n",
    "* *juv_fel_count*. The number of prior charges for juvenile fellonies up to but not including the current offense.\n",
    "* *juv_misd_count*. The number of prior charges for juvenile misdemeanors up to but not including the current offense.\n",
    "* *juv_other_count*. The number of prior charges for other juvenile offenses up to but not including the current offense.\n",
    "* *sex_Male*. The defendant's sex, measured as US census sex categories (either 1 for *Male* or 0 for *Female*).\n",
    "* *race*. The defendant's race, measured as an adapted version of US census race categories (either 1 for *Caucasian* or 0 for *African-American*.\n",
    "\n",
    "The proportion of positives in the data set is almost 0.5, which means that the dataset is balanced in terms of positives/negatives. This makes **accuracy** a (somewhat) suitable metric for measuring the overall predictive performance of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalized Odds\n",
    "In this tutorial, we will attempt to train a **new classifier** for predicting recidivism with similar error rates across racial groups. In this way, we hope to avoid potential *quality-of-service harm*. \n",
    "\n",
    "> **Quality-of-service harm**: the algorithm makes more mistakes for some groups than for others. For example, in a hiring scenario, we may mistakingly reject strong female candidates more often than strong male candidates. \n",
    "\n",
    "The risk of quality-of-service harm is particularly prevalent if the relationship between the features and target variable is different across groups. The risk is further amplified if less data is available for some groups. We will measure potential quality-of-service harm using the fairness criterion *equalized odds*.\n",
    "\n",
    "> **Equalized Odds** holds if, for all values of $y$ and $a$, $$P(\\hat{Y} = y | A = a, Y = y) = P(\\hat{Y} = y | A = a', Y = y)$$ where $\\hat{Y}$ is the output of our model, $Y$ the observed outcome, and $A$ the set of sensitive characteristics.\n",
    "\n",
    "In other words, the **false positive rate** and **false negative rate** should be equal across groups. As explained before, a false positive prediction in pre-trial risk assessment can have large consequences for the involved defendant, as they may have to await trial in jail. This may even result in the defendant pleading guilty to a crime they did not commit. On the other extreme, false negatives could mean that dangerous individuals are released into society."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score(\n",
    "    y_train,\n",
    "    y_train_pred,\n",
    "    y_test,\n",
    "    y_test_pred,\n",
    "    sensitive_features_train,\n",
    "    sensitive_features_test,\n",
    "    metrics={\"accuracy\": accuracy_score, \"fpr\": false_positive_rate, \"fnr\": false_negative_rate,},\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function to evaluate classifiers without too much repetition of code.\n",
    "    \"\"\"\n",
    "\n",
    "    # training set\n",
    "    mf_train = MetricFrame(\n",
    "        metrics=metrics,\n",
    "        y_true=y_train,\n",
    "        y_pred=y_train_pred,\n",
    "        sensitive_features=sensitive_features_train,\n",
    "    )\n",
    "\n",
    "    # test set\n",
    "    mf_test = MetricFrame(\n",
    "        metrics=metrics,\n",
    "        y_true=y_test,\n",
    "        y_pred=y_test_pred,\n",
    "        sensitive_features=sensitive_features_test,\n",
    "    )\n",
    "\n",
    "    # display results\n",
    "    display(\n",
    "        pd.concat(\n",
    "            [mf_train.by_group, mf_test.by_group], keys=[\"train\", \"test\"]\n",
    "        ).unstack(level=0)\n",
    "    )\n",
    "\n",
    "    # compute metrics\n",
    "    print(\n",
    "        \"equalized odds (test): %.2f\"\n",
    "        % equalized_odds_difference(\n",
    "            y_true=y_test,\n",
    "            y_pred=y_test_pred,\n",
    "            sensitive_features=X_test[\"race_Caucasian\"],\n",
    "        )\n",
    "    )\n",
    "    print(\"accuracy (test): %.2f\" % accuracy_score(y_true=y_test, y_pred=y_test_pred))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification without Fairness Constraints\n",
    "\n",
    "First, let's train a simple logistic regression classifier on the data and see how it performs.\n",
    "\n",
    "**NOTE**. *If we were building a model to be put in production, we would encourage you to use a separate validation and test set. However, for the sake of simplicity, we only consider train and test set in this tutorial.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Exercise*: train a `LogisticRegression` model on the data without fairness constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train simple logistic regression model\n",
    "lr = LogisticRegression()\n",
    "lr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "score(\n",
    "    y_train,\n",
    "    lr.predict(X_train),\n",
    "    y_test,\n",
    "    lr.predict(X_test),\n",
    "    X_train[\"race_Caucasian\"],\n",
    "    X_test[\"race_Caucasian\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set accuracy of our classifier is approximately the same for African-Americans (0.69) and Caucasians (0.67). However, the FNR and FPR differ substantially, indicating that African-Americans are more often falsely predicted to recidivise, whereas Caucasians are more often falsely predicted to *not* recidivise. This disparity is reflected in the computed equalized odds difference (0.32).\n",
    "\n",
    "> #### Intermezzo: Equalized Odds and Construct Validity\n",
    "> In what scenarios should we use equalized odds as a fairness metric? Equalized odds quantifies the understanding of fairness that we should not make more mistakes for some groups than for other groups. Importantly, **equalized odds implicitly assumes that the target variable is a good representation of what we are actually interested in**: error rates are only meaningful if the ground-truth target variable is an unbiased measurement of the thing we are trying to predict.\n",
    ">\n",
    "> **Construct validity** is a concept from the social sciences that refers to *the extent to which a measurement actually measures the phenomenon we are trying to measure*. In the context of fairness, [a lack of construct validity in the target variable can be a source of downstream model unfairness](https://arxiv.org/abs/1912.05511).\n",
    "> * [Healthcare costs can be a biased measurement of healthcare needs](https://science.sciencemag.org/content/366/6464/447.abstract), as costs may reflect patients' economic circumstances rather than their health\n",
    "> * Historical hiring decisions are not necessarily equivalent to historical employee quality, due to systemic and/or (unconscious) social biases in the hiring process.\n",
    "> * Observed fraud is only a subsample of actual fraud. If potential cases of fraud are not selected randomly, there is a risk of selection bias. If the selection biass is associated with sensitive group membership, some groups may be overscrutinized causing the observed fraud rate to be inflated.\n",
    "\n",
    "Although we use equalized odds as a fairness constraint, we would like to acknowledge that **due to biased policing practices, re-arrests is likely a biased measurement of recidivism.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness-Aware Machine Learning\n",
    "There exist several technical approaches to explicitly incorporate fairness constraints in a machine learning model. We can roughly distinguish three types of approaches:\n",
    "* **Pre-processing** algorithms adjust training data directly to mitigate downstream model unfairness. \n",
    "* **Constrained learning** approaches incorporate fairness constraints into the machine learning process, either by directly incorporating a constraint in the loss function or by learning an ensemble of predictors.\n",
    "* **Post-processing** techniques make adjustments to existing machine learning models to satisfy fairness constraints, either by adjusting the parameters of a trained model directly or by post-processing the predictions of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Post-Processing the Decision Threshold\n",
    "In our previous model, we have simply applied the default decision threshold of 0.5 for both African-Americans and Caucasians. One way to account for differences in FPR and FNR is to **choose a separate threshold for each sensitive group**. Note that this implies that **different groups are held to a different standard**.\n",
    "\n",
    "We will now choose a new decision threshold for African-Americans to ensure that the FPR of African-Americans is equal to the FPR of Caucasians. In our previous model, the FPR in the training set is approximately 0.13 for Caucasians. We use this as our target FPR for African-American defendants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make scoring predictions\n",
    "y_train_score_lr = pd.Series(lr.predict_proba(X_train)[:, 1], index=X_train.index)\n",
    "y_test_score_lr = pd.Series(lr.predict_proba(X_test)[:, 1], index=X_test.index)\n",
    "\n",
    "# get indices for grouping\n",
    "idx_AA_train = X_train[X_train[\"race_Caucasian\"] == 0].index\n",
    "\n",
    "# get new thresholds based on ROC curve for AA\n",
    "fpr_AA_train, tpr_AA_train, thresholds_AA_train = roc_curve(\n",
    "    y_true=y_train[idx_AA_train], y_score=y_train_score_lr[idx_AA_train]\n",
    ")\n",
    "new_thr = thresholds_AA_train[(np.abs(fpr_AA_train - 0.13)).argmin()]\n",
    "\n",
    "print(\"New decision threshold for African-Americans: %.2f\" % new_thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the training data, we have computed that a decision threshold of 0.62 would lead to a FPR of approximately 0.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get remaining indices for grouping\n",
    "idx_AA_test = X_test[X_test[\"race_Caucasian\"] == 0].index\n",
    "idx_C_test = X_test[X_test[\"race_Caucasian\"] == 1].index\n",
    "idx_C_train = X_train[X_train[\"race_Caucasian\"] == 1].index\n",
    "\n",
    "# plot ROC curves\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = plt.gca()\n",
    "\n",
    "RocCurveDisplay.from_estimator(\n",
    "    lr,\n",
    "    X_test.loc[idx_AA_test, :],\n",
    "    y_test.loc[idx_AA_test],\n",
    "    name=\"African-Americans\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "RocCurveDisplay.from_estimator(\n",
    "    lr,\n",
    "    X_test.loc[idx_C_test, :],\n",
    "    y_test.loc[idx_C_test],\n",
    "    name=\"Caucasian\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# add thresholds\n",
    "plt.scatter(\n",
    "    false_positive_rate(\n",
    "        y_true=y_test[idx_AA_test], y_pred=y_test_score_lr[idx_AA_test] >= 0.5\n",
    "    ),\n",
    "    true_positive_rate(\n",
    "        y_true=y_test[idx_AA_test], y_pred=y_test_score_lr[idx_AA_test] >= 0.5\n",
    "    ),\n",
    "    label=\"Threshold 0.50 (African-American)\",\n",
    ")\n",
    "plt.scatter(\n",
    "    false_positive_rate(\n",
    "        y_true=y_test[idx_C_test], y_pred=y_test_score_lr[idx_C_test] >= 0.5\n",
    "    ),\n",
    "    true_positive_rate(\n",
    "        y_true=y_test[idx_C_test], y_pred=y_test_score_lr[idx_C_test] >= 0.5\n",
    "    ),\n",
    "    label=\"Threshold 0.50 (Caucasian)\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    false_positive_rate(\n",
    "        y_true=y_test[idx_AA_test], y_pred=y_test_score_lr[idx_AA_test] >= new_thr\n",
    "    ),\n",
    "    true_positive_rate(\n",
    "        y_true=y_test[idx_AA_test], y_pred=y_test_score_lr[idx_AA_test] >= new_thr\n",
    "    ),\n",
    "    label=\"Threshold %.2f (African-American)\" % new_thr,\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot visualizes the results of our manual selection approach on the test data. We have plotted the ROC curve for each racial group separately. The original decision threshold of 0.5 resulted in higher TPR and FPR for African-Americans. By choosing a different threshold for this group, we end up on a point on the ROC curve that is much closer to the FPR and TPR of Caucasians.\n",
    "\n",
    "Now let's evaluate the performance of the new decision threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make new predictions at two separate decision thresholds\n",
    "y_train_pred_lr_m = pd.concat(\n",
    "    [y_train_score_lr[idx_AA_train] >= new_thr, y_train_score_lr[idx_C_train] >= 0.5]\n",
    ").reindex(X_train.index)\n",
    "y_test_pred_lr_m = pd.concat(\n",
    "    [y_test_score_lr[idx_AA_test] >= new_thr, y_test_score_lr[idx_C_test] >= 0.5]\n",
    ").reindex(X_test.index)\n",
    "\n",
    "# score\n",
    "score(\n",
    "    y_train,\n",
    "    y_train_pred_lr_m,\n",
    "    y_test,\n",
    "    y_test_pred_lr_m,\n",
    "    X_train[\"race_Caucasian\"],\n",
    "    X_test[\"race_Caucasian\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from this table, we have been able to substantially reduce equalized odds difference (decreased from 0.32 to 0.05), with similar overall accuracy (decreased from 0.68 to 0.65)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Post-Processing Using `ThresholdOptimizer`\n",
    "In addition to manual optimization, it is also possible to automatically determine group-specific decision thresholds. The class `fairlearn.postprocessing.ThresholdOptimizer` is based on the algorithm introduced by [Hardt. et al (2016)](https://papers.nips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html). \n",
    "\n",
    "> #### Group-Specific Randomized Thresholds\n",
    "> Equalized odds requires us to set the false positive rate and true positive rate equal across groups. Through group-specific decision thresholds, we can take any point on the group-specific ROC curve, which increases allows us to get a more similar FPR and FNR for both groups. However, group-specific thresholds still limit us to the (FPR, TPR) combinations that lie on the intersection of the group-specific ROC curves. In some cases, the group-specific ROC curves may not intersect or represent a poor trade-off between false positives and false negatives. \n",
    ">\n",
    "> To further increase the solution space, the `ThresholdOptimizer` allows the group-specific decision thresholds to be **randomized**: we randomly pick between two distinct thresholds.\n",
    ">\n",
    "> <img src=\"randomizedthreshold.pdf\" alt=\"a randomized decision threshold me\" width=\"300\"/>\n",
    ">\n",
    ">  The probability $p_a$ with which we choose one threshold over the other determines which (fpr,tpr) combination in the ROC space we end up with. How this works is the easiest to understand through a visualization. In the plot below, the solid curve represents the group-specific ROC curve of the group that is best-off. By randomizing between the two thresholds, we can achieve any (FPR,TPR) combination on the line between the two thresholds.\n",
    "> \n",
    "> <img src=\"roccurve.pdf\" alt=\"randomization allows us to achieve any fpr/tpr combination  under the group-specific ROC-curve.\" width=\"300\"/>\n",
    ">\n",
    "> By carefully selecting the two decision thresholds and probability $p_a$, we can end up with a combination that is also on the group-specific ROC-curve of the worst-off group, satisfying equalized odds. The point on the ROC curve at which randomization is aimed, is optimized such that the disparity is the smallest, while predictive performance is the highest.\n",
    ">\n",
    "> Note that using this algorithm has the following implications:\n",
    "> * The predictive performance for each group is decreased until it is equal to that of the worst-off group.\n",
    "> * Due to randomization, two individuals with the exact same characteristics may receive a different classification.\n",
    "\n",
    "\n",
    "The `ThresholdOptimizer` class has the following parameters: \n",
    "* `estimator`: the (fitted) classifier\n",
    "* `constraints` : the fairness constraint for which we want to optimize, we can choose between `'demographic_parity'`, `'equalized odds'`, `'false_positive_rate_parity'` `'false_negative_rate_parity'`, `'true_positive_parity'` and `'true_negative_parity'`.\n",
    "* `objective` : the predictive performance objective under which threshold optimization is performed. Not all objectives are allowed for all types of constraints. Possible inputs are: `'accuracy_score'`, `'balanced_accuracy_score'` (for all constraint types) and `'selection_rate'`, `'true_positive_rate'`, `'true_negative_rate'` (for all constraint types except `'equalized odds'`).\n",
    "* `grid_size` : the values of the constraint metric are discretized according to the grid of the specified size over the interval [0,1]. The optimization is performed with respect to the constraints achieving those values. In case of `'equalized_odds'` the constraint metric is the false positive rate.\n",
    "* `prefit` : if `True`, avoid refitting the given estimator.\n",
    "* `predict_method` : defines which method of the estimator is used to get the output values.\n",
    "\n",
    "The methods of the `ThresholdOptimizer` are similar to the familiar scikit-learn API, with the addition of `sensitive_features`. This must be is a list-like object (e.g., a numpy array or pandas series) that represents sensitive group-membership.\n",
    "* `fit(X, y, *, sensitive_features, **kwargs)` (any `**kwargs` will be passed to the `fit()` method of the `estimator`)\n",
    "* `predict(X, *, sensitive_features, random_state=None)` (`random_state` can be used to get reproducible results).\n",
    "\n",
    "Let's see how this algorithm does on our existing logistic regression classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Exercise*: use `ThresholdOptimizer` to post-process the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train thresholdoptimizer\n",
    "to = ThresholdOptimizer(\n",
    "    estimator=,\n",
    "    constraints=,\n",
    "    predict_method=,\n",
    "    objective=,\n",
    ")\n",
    "to.fit(X_train, y_train, sensitive_features=])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "score(\n",
    "    y_train,\n",
    "    to.predict(X_train, sensitive_features=X_train[\"race_Caucasian\"], random_state=0),\n",
    "    y_test,\n",
    "    to.predict(X_test, sensitive_features=X_test[\"race_Caucasian\"], random_state=0),\n",
    "    X_train[\"race_Caucasian\"],\n",
    "    X_test[\"race_Caucasian\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equalized odds difference is even lower compared to our manual decision threshold optimization, the accuracy is similar. Although the equalized odds difference is similar, we end up at a different spot on the ROC-curve, with a slightly higher FPR and lower FNR compared to our manual optimization.\n",
    "\n",
    "To better understand the results of the `ThresholdOptimizer`, we can visulaize the solution using `fairlearn.postprocessing.plot_threshold_optimizer()`. The plot shows the group-specific ROC curves and visualizes their overlap. As we've seen before, the group-specific ROC curves hardly interesect, apart from trivial end-points. The solution found in the optimization is slightly different from our manual group-specific threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print solution\n",
    "print(to.interpolated_thresholder_)\n",
    "\n",
    "# plot solution\n",
    "plot_threshold_optimizer(to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For African-American defendants the decision threshold is randomized between approximately 0.59 ($p=0.69$) and 0.54 ($p=0.31$). For Caucasian defendants the `ThresholdOptimizer` randomizes between a decision threshold of 0.46 ($p=0.992$) and 0.43 ($p=0.008$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Learning using `ExponentiatedGradient`\n",
    "\n",
    "We can also directly take into account the fairness constraint during model training. One such approach is introduced by [Agarwal et al. (2018)](https://proceedings.mlr.press/v80/agarwal18a.html) which is implemented in Fairlearn's `fairlearn.reductions.ExponentiatedGradient`.\n",
    "\n",
    "\n",
    "> #### A Reductions Approach to Fairness\n",
    "> We can think about a reductions approach as a 'wrapper' around a machine learning algorithm (or `estimator` in scikit-learn terminology). The intuition behind the reductions approach in `ExponentiatedGradient` is to learn an ensemble of cost-sensitive classifiers, where each learned classifier is represents a different trade-off between the classification error and a fairness constraint. Each of these trade-offs is achieved by reweighing the training data set. At deployment time, `ExponentiatedGradient` randomizes between the classifiers in the ensemble, i.e., it selects one of the classifiers according to a set of predefined probability weights. This randomization scheme is selected such that the classification error is minimized under the fairness constraint. The name `ExponentiatedGradient` refers to the optimization approach that is used to solve this optimization problem: exponentiated gradient descent.\n",
    ">\n",
    "> Using this algorithm has the following implications\n",
    "> * As opposed to `ThresholdOptimizer`, `ExponentiatedGradient` does not require access to sensitive features at training time.\n",
    "> * Similar to `ThresholdOptimizer`, `ExponentiatedGradient` uses randomization, meaning that two individuals with the exact same characteristics can receive the same prediction.\n",
    "> * By learning an ensemble of classifiers, we sacrifice interpretability of the underlying estimator.\n",
    "\n",
    "The `ExponentiatedGradient` class has the following parameters:\n",
    "* `estimator`: a (scikit-learn) estimator with a `fit(X, y, sample_weight)` and `predict(X)` method.\n",
    "* `constraints` : the fairness constraints expressed as `Moment`s\n",
    "* `eps` : allowed fairness constraint violation; the solution is guaranteed to have the error within 2*best_gap of the best error under constraint eps; the constraint violation is at most 2*(eps+best_gap).\n",
    "* `max_iter`: Maximum number of iterations\n",
    "* `nu` : convergence threshold\n",
    "* `eta_0` : initial setting of the learning rate\n",
    "* `run_linprog_step` : if True each step of exponentiated gradient is followed by the saddle point optimization over the convex hull of classifiers returned so far; default True\n",
    "* `sample_weight_name` : name of the argument to estimator.fit() which supplies the sample weights (defaults to `'sample_weight'`)\n",
    "\n",
    "\n",
    "And the familiar methods:\n",
    "* `fit(X, y, sensitive_features)`\n",
    "* `predict(X, random_state=None)`\n",
    "\n",
    "To use the `ExponentiatedGradient` we need to define the constraint in the form of a `Moment`. In the case of equalized odds, this can be achieved using the `fairlearn.metrics.EqualizedOdds` moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Exercise*: train `ExponentiatedGradient` on the training data with a `LogisticRegression` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "eg = ExponentiatedGradient(\n",
    "    estimator=, \n",
    "    constraints=, \n",
    "    eps=,\n",
    ")\n",
    "eg.fit(X=X_train, y=y_train, sensitive_features=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "score(\n",
    "    y_train,\n",
    "    eg.predict(X_train, random_state=0),\n",
    "    y_test,\n",
    "    eg.predict(X_test, random_state=0),\n",
    "    X_train[\"race_Caucasian\"],\n",
    "    X_test[\"race_Caucasian\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding Remarks\n",
    "In this tutorial we have showcased several ways in which we can use fairness-aware machine learning algorithms to post-process or learn classifiers that adhere to a fairness constraint. In particular, we considered:\n",
    "* post-processing predictions through manually identifying a group-specific decision threshold\n",
    "* post-processing predictions through automatically identifying the optimal randomized group-specific decision thresholds using `ThresholdOptimizer`\n",
    "* taking into account a fairness constraint during model training using `ExponentiatedGradient`\n",
    "\n",
    "### Discussion Points\n",
    "Here are several discussion points I would like to invite you to think about.\n",
    "\n",
    "#### Connection to fairness metrics\n",
    "* Fairness-aware pre-processing algorithms typically optimize for demographic parity, not for equalized odds or equal calibration. Can you explain why?\n",
    "* How does post-processing for equalized odds affects other notions of fairness, such as equal calibration and demographic parity? As an exercise, compute fairness metrics that represent alternative notions of (group) fairness. Can you explain the results?\n",
    "\n",
    "#### The justification of fair-ml\n",
    "* Our manual choice of the decision threshold uses a different decision threshold for African-Americans than for Caucasians. Under which circumstances would you deem such a policy fair, if any? Why?\n",
    "* The post-processing algorithm implemented in `ThresholdOptimizer` uses randomization in order to achieve equalized odds. Consequently, individuals with the exact same characteristics may receive a different prediction. Under which circumstances would you deem such a policy fair, if any? Why? *For the interested reader, I have tried to answer this question in [this paper](https://arxiv.org/abs/2202.08536).*\n",
    "* The constrained learning algorithm implemented in `ExponentiatedGradient` trains an ensemble of classifiers. This vastly decreases the interpretabiltiy of the final model compared to plain logistic regression. Under which circumstances would you be willing to sacrifice interpretability for fairness, if at all? Why? *If you want to think more about the relationship between fairness interpretability you may find [this paper](https://hdsr.mitpress.mit.edu/pub/7z10o269/release/4) an interesting read*.\n",
    "\n",
    "#### Beyond fair-ml\n",
    "* In this tutorial, we have focussed on technical approaches to optimize for equalized odds. Can you think of other actions in the machine learning development process that could be used to enhance the predictive performance across groups, e.g., during problem formulation and data collection?\n",
    "* In the United States, pretrial risk assessment tools are not used for automated decision-making. Instead, they are used as decision support for judges. Does this influence the way in which you would evaluate the fairness of a model in practice? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
